{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BPE Training Project\\n\n",
        "## Complete Implementation in English\\n\\n\n",
        "This notebook consolidates all project files:\\n\n",
        "1. bpetraining.py\\n\n",
        "2. Tokenizer.py\\n\n",
        "3. problem4.py\\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## bpetraining.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "BPE training implementation.\n",
        "\n",
        "Further-optimized version:\n",
        "- Builds initial pair -> total count and pair -> set(word_indices)\n",
        "- On each merge, only touch words that contain the chosen pair:\n",
        "  * remove their old pair contributions\n",
        "  * apply the merge to that word\n",
        "  * add new pair contributions for that word\n",
        "- Avoids rebuilding global pair counts from scratch each iteration.\n",
        "- Keeps same API as original: train_bpe(input_path, vocab_size, special_tokens)\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "import regex as re\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "# GPT-2 regex pattern (as required by the spec)\n",
        "PAT = r\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\n",
        "\n",
        "_token_re = re.compile(PAT)\n",
        "\n",
        "def _split_on_specials(text: str, special_tokens: List[str]) -> List[str]:\n",
        "    \"\"\"Split text into chunks that do not cross special token boundaries.\n",
        "    The special tokens themselves are kept as chunks.\"\"\"\n",
        "    if not special_tokens:\n",
        "        return [text]\n",
        "    esc = [re.escape(st) for st in special_tokens]\n",
        "    splitter = re.compile(\"(\" + \"|\".join(esc) + \")\")\n",
        "    parts = splitter.split(text)\n",
        "    return [p for p in parts if p != \"\"]\n",
        "\n",
        "def _pretokenize_chunk(chunk: str, special_tokens_set: set) -> List[Tuple[bytes, ...]]:\n",
        "    \"\"\"Pre-tokenize a chunk. Protected special tokens are returned as single tokens.\n",
        "    Otherwise apply GPT-2 regex and return tuple-of-single-byte-bytes symbols for each token.\"\"\"\n",
        "    if chunk in special_tokens_set:\n",
        "        return [(chunk.encode(\"utf-8\"),)]\n",
        "\n",
        "    out = []\n",
        "    for m in _token_re.finditer(chunk):\n",
        "        tok = m.group(0)\n",
        "        if not tok:\n",
        "            continue\n",
        "        b = tok.encode(\"utf-8\")\n",
        "        symbols = tuple(bytes([bb]) for bb in b)\n",
        "        if symbols:\n",
        "            out.append(symbols)\n",
        "    return out\n",
        "\n",
        "def train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]):\n",
        "    \"\"\"Train BPE tokenizer.\n",
        "\n",
        "    Returns:\n",
        "      vocab: dict[int, bytes]  -- mapping token id -> byte sequence (as bytes)\n",
        "      merges: list[tuple[bytes, bytes]] -- list of merges in creation order\n",
        "    \"\"\"\n",
        "    p = Path(input_path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"input file not found: {input_path}\")\n",
        "\n",
        "    text = p.read_text(encoding=\"utf-8\")\n",
        "\n",
        "    # split on special tokens first (they are protected)\n",
        "    chunks = _split_on_specials(text, special_tokens)\n",
        "    special_set = set(special_tokens)\n",
        "\n",
        "    # pretokenize all chunks and count frequencies of \"words\" (tuples of symbols)\n",
        "    word_freq: Counter = Counter()\n",
        "    for chunk in chunks:\n",
        "        tokens = _pretokenize_chunk(chunk, special_set)\n",
        "        for t in tokens:\n",
        "            word_freq[t] += 1\n",
        "\n",
        "    # Initialize vocab list (bytes sequences)\n",
        "    vocab_list: List[bytes] = []\n",
        "    # Add special tokens first as full-byte sequences\n",
        "    for st in special_tokens:\n",
        "        vocab_list.append(st.encode(\"utf-8\"))\n",
        "    # Add 256 single-byte tokens\n",
        "    for i in range(256):\n",
        "        vocab_list.append(bytes([i]))\n",
        "\n",
        "    merges: List[Tuple[bytes, bytes]] = []\n",
        "\n",
        "    # Convert word_freq to mutable list-of-lists + freqs\n",
        "    words: List[List[bytes]] = [list(w) for w in word_freq.keys()]  # each w: list of symbols (bytes)\n",
        "    freqs: List[int] = list(word_freq.values())\n",
        "    n_words = len(words)\n",
        "\n",
        "    # Build initial pair_counts and mapping pair -> set(word_indices)\n",
        "    pair_counts: Dict[Tuple[bytes, bytes], int] = defaultdict(int)\n",
        "    pair_to_word_indices: Dict[Tuple[bytes, bytes], set] = defaultdict(set)\n",
        "\n",
        "    for idx, (w, f) in enumerate(zip(words, freqs)):\n",
        "        ln = len(w)\n",
        "        for i in range(ln - 1):\n",
        "            pair = (w[i], w[i + 1])\n",
        "            pair_counts[pair] += f\n",
        "            pair_to_word_indices[pair].add(idx)\n",
        "\n",
        "    current_vocab_size = len(vocab_list)\n",
        "\n",
        "    # Main loop: merge until target vocab_size\n",
        "    while current_vocab_size < vocab_size and pair_counts:\n",
        "        # Find best pair: highest freq, tie-break by lexicographic greater pair\n",
        "        # Note: iterating over dict items is fine; number of distinct pairs is typically modest\n",
        "        best_pair = None\n",
        "        best_freq = -1\n",
        "        for pair, cnt in pair_counts.items():\n",
        "            if cnt > best_freq or (cnt == best_freq and (best_pair is None or pair > best_pair)):\n",
        "                best_pair = pair\n",
        "                best_freq = cnt\n",
        "\n",
        "        if best_pair is None or best_freq <= 0:\n",
        "            break\n",
        "\n",
        "        a, b = best_pair\n",
        "        new_symbol = a + b\n",
        "\n",
        "        # Record merge and add to vocab\n",
        "        merges.append((a, b))\n",
        "        vocab_list.append(new_symbol)\n",
        "        current_vocab_size += 1\n",
        "\n",
        "        # Get affected word indices (copy because we'll modify the sets)\n",
        "        affected_indices = list(pair_to_word_indices.get(best_pair, set()))\n",
        "        if not affected_indices:\n",
        "            # no words actually contain it any more (defensive)\n",
        "            # remove pair and continue\n",
        "            pair_counts.pop(best_pair, None)\n",
        "            pair_to_word_indices.pop(best_pair, None)\n",
        "            continue\n",
        "\n",
        "        # For each affected word: remove old pair contributions, modify word, then add new pair contributions\n",
        "        for idx in affected_indices:\n",
        "            # If this word was removed/empty somehow, skip\n",
        "            if idx >= len(words):\n",
        "                continue\n",
        "            w = words[idx]\n",
        "            f = freqs[idx]\n",
        "            if len(w) < 2:\n",
        "                # nothing to do\n",
        "                continue\n",
        "\n",
        "            # Compute old pairs for this word (list)\n",
        "            old_pairs = []\n",
        "            for i in range(len(w) - 1):\n",
        "                old_pairs.append((w[i], w[i + 1]))\n",
        "\n",
        "            # Remove this word's contribution from global pair_counts and pair_to_word_indices\n",
        "            # (for each old pair, decrement and remove idx from set)\n",
        "            for pair in old_pairs:\n",
        "                # decrement count\n",
        "                cnt = pair_counts.get(pair, 0)\n",
        "                if cnt <= f:\n",
        "                    # remove entirely\n",
        "                    pair_counts.pop(pair, None)\n",
        "                else:\n",
        "                    pair_counts[pair] = cnt - f\n",
        "                # remove index from mapping set\n",
        "                s = pair_to_word_indices.get(pair)\n",
        "                if s:\n",
        "                    s.discard(idx)\n",
        "                    if len(s) == 0:\n",
        "                        pair_to_word_indices.pop(pair, None)\n",
        "\n",
        "            # Apply merge on this word: replace adjacent (a,b) with new_symbol\n",
        "            new_w = []\n",
        "            i = 0\n",
        "            changed = False\n",
        "            while i < len(w):\n",
        "                if i < len(w) - 1 and w[i] == a and w[i + 1] == b:\n",
        "                    new_w.append(new_symbol)\n",
        "                    i += 2\n",
        "                    changed = True\n",
        "                else:\n",
        "                    new_w.append(w[i])\n",
        "                    i += 1\n",
        "\n",
        "            # If no change (maybe pair no longer present due to earlier merges), skip adding contributions\n",
        "            words[idx] = new_w\n",
        "\n",
        "            if not changed:\n",
        "                continue\n",
        "\n",
        "            # Compute new pairs for this modified word and add contributions\n",
        "            ln2 = len(new_w)\n",
        "            for j in range(ln2 - 1):\n",
        "                p = (new_w[j], new_w[j + 1])\n",
        "                pair_counts[p] += f\n",
        "                pair_to_word_indices[p].add(idx)\n",
        "\n",
        "        # Finally, remove the merged pair from maps if present (it's now obsolete)\n",
        "        pair_counts.pop(best_pair, None)\n",
        "        pair_to_word_indices.pop(best_pair, None)\n",
        "\n",
        "    # Build final vocab mapping id -> bytes\n",
        "    vocab: Dict[int, bytes] = {i: v for i, v in enumerate(vocab_list)}\n",
        "    return vocab, merges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import regex \n",
        "from typing import Dict, List, Tuple, Any, Union\n",
        "import json\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "class OptimizedBPE:\n",
        "    def __init__(self, vocab_size: int = 1000,\n",
        "                 special_tokens: List[str] = None,\n",
        "                 lowercase: bool = False):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.lowercase = lowercase\n",
        "        self.special_tokens = special_tokens or []\n",
        "        self.gpt2_pattern = regex.compile(\n",
        "            r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
        "        )\n",
        "\n",
        "        self.vocab: Dict[int, bytes] = {}  # id -> bytes\n",
        "        self.merges: List[Tuple[bytes, bytes]] = []  \n",
        "        self.id_to_token: Dict[int, bytes] = {}\n",
        "        self.token_to_id: Dict[bytes, int] = {}\n",
        "        self.str_vocab: Dict[str, int] = {}\n",
        "        self.id_to_str_token: Dict[int, str] = {}\n",
        "        self.special_token_bytes: Dict[str, bytes] = {}\n",
        "        self.special_token_ids: Dict[str, int] = {}\n",
        "        self._cache: Dict[str, List[str]] = {}\n",
        "\n",
        "    def initialize_from_existing(self, vocab: Dict[int, bytes], merges: List[Tuple[bytes, bytes]]):\n",
        "        self.vocab = vocab\n",
        "        self.merges = merges\n",
        "        self.id_to_token = vocab\n",
        "        self.token_to_id = {v: k for k, v in vocab.items()}\n",
        "        self.str_vocab = {}\n",
        "        self.id_to_str_token = {}\n",
        "        for token_id, token_bytes in vocab.items():\n",
        "            try:\n",
        "                token_str = token_bytes.decode('utf-8')\n",
        "            except UnicodeDecodeError:\n",
        "                token_str = token_bytes.hex()\n",
        "            self.str_vocab[token_str] = token_id\n",
        "            self.id_to_str_token[token_id] = token_str\n",
        "\n",
        "        for token in self.special_tokens:\n",
        "            token_bytes = token.encode('utf-8')\n",
        "            self.special_token_bytes[token] = token_bytes\n",
        "\n",
        "            if token_bytes in self.token_to_id:\n",
        "                self.special_token_ids[token] = self.token_to_id[token_bytes]\n",
        "            else:\n",
        "                new_id = max(self.vocab.keys()) + 1 if self.vocab else 0\n",
        "                self.vocab[new_id] = token_bytes\n",
        "                self.id_to_token[new_id] = token_bytes\n",
        "                self.token_to_id[token_bytes] = new_id\n",
        "                self.special_token_ids[token] = new_id\n",
        "                self.str_vocab[token] = new_id\n",
        "                self.id_to_str_token[new_id] = token\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        if self.special_tokens:\n",
        "            sorted_special_tokens = sorted(self.special_tokens, key=len, reverse=True)\n",
        "            escaped = [re.escape(t) for t in sorted_special_tokens]\n",
        "            special_pattern = re.compile(f\"({'|'.join(escaped)})\")\n",
        "            parts = special_pattern.split(text)\n",
        "        else:\n",
        "            parts = [text]\n",
        "\n",
        "        all_ids = []\n",
        "        for part in parts:\n",
        "            if not part:\n",
        "                continue\n",
        "\n",
        "            if part in self.special_token_ids:\n",
        "                all_ids.append(self.special_token_ids[part])\n",
        "            else:\n",
        "                pretokens = self.gpt2_pattern.findall(part)\n",
        "                for pretoken in pretokens:\n",
        "                    ids = self._apply_bpe_to_pretoken(pretoken)\n",
        "                    all_ids.extend(ids)\n",
        "\n",
        "        return all_ids\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        ids = self.encode(text)\n",
        "\n",
        "        tokens = []\n",
        "        for token_id in ids:\n",
        "            if token_id in self.id_to_str_token:\n",
        "                tokens.append(self.id_to_str_token[token_id])\n",
        "            elif token_id in self.id_to_token:\n",
        "                token_bytes = self.id_to_token[token_id]\n",
        "                try:\n",
        "                    tokens.append(token_bytes.decode('utf-8'))\n",
        "                except UnicodeDecodeError:\n",
        "                    tokens.append(token_bytes.hex())\n",
        "            else:\n",
        "                tokens.append(f\"[UNK:{token_id}]\")\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def _apply_bpe_to_pretoken(self, pretoken: str) -> List[int]:\n",
        "        if not pretoken:\n",
        "            return []\n",
        "\n",
        "        pretoken_bytes = pretoken.encode('utf-8')\n",
        "\n",
        "        tokens = [bytes([b]) for b in pretoken_bytes]\n",
        "\n",
        "        for a, b in self.merges:\n",
        "            i = 0\n",
        "            new_tokens = []\n",
        "            while i < len(tokens):\n",
        "                if i < len(tokens) - 1 and tokens[i] == a and tokens[i + 1] == b:\n",
        "                    new_tokens.append(a + b)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_tokens.append(tokens[i])\n",
        "                    i += 1\n",
        "            tokens = new_tokens\n",
        "\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            if token in self.token_to_id:\n",
        "                ids.append(self.token_to_id[token])\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int]) -> str:\n",
        "        if not ids:\n",
        "            return \"\"\n",
        "\n",
        "        flat_ids = []\n",
        "        for item in ids:\n",
        "            if isinstance(item, list):\n",
        "                flat_ids.extend(item)\n",
        "            else:\n",
        "                flat_ids.append(item)\n",
        "\n",
        "        result_bytes = b''\n",
        "        for token_id in flat_ids:\n",
        "            if token_id in self.id_to_token:\n",
        "                result_bytes += self.id_to_token[token_id]\n",
        "\n",
        "        try:\n",
        "            return result_bytes.decode('utf-8')\n",
        "        except UnicodeDecodeError:\n",
        "            return result_bytes.decode('utf-8', errors='replace')\n",
        "\n",
        "    def encode_iterable(self, texts: Any) -> List[List[int]]:\n",
        "        if hasattr(texts, 'read'):\n",
        "            content = texts.read()\n",
        "            lines = content.splitlines(keepends=False)\n",
        "            text_list = [line for line in lines if line]\n",
        "        elif isinstance(texts, list):\n",
        "            text_list = texts\n",
        "        else:\n",
        "            text_list = list(texts)\n",
        "\n",
        "        all_ids = []\n",
        "        for text in text_list:\n",
        "            if text:\n",
        "                ids = self.encode(text)\n",
        "                all_ids.extend(ids)\n",
        "\n",
        "        return [all_ids]\n",
        "\n",
        "    def save(self, path: str):\n",
        "        data = {\n",
        "            'vocab': self.vocab,\n",
        "            'merges': self.merges,\n",
        "            'special_tokens': self.special_tokens,\n",
        "        }\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(data, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path: str) -> 'OptimizedBPE':\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        bpe = cls(\n",
        "            vocab_size=len(data['vocab']),\n",
        "            special_tokens=data['special_tokens']\n",
        "        )\n",
        "        bpe.initialize_from_existing(data['vocab'], data['merges'])\n",
        "        return bpe\n",
        "\n",
        "\n",
        "def test_gpt2_compatibility():\n",
        "    vocab = {\n",
        "        0: b'<|endoftext|>',\n",
        "        82: b's',\n",
        "        198: b'Hello',\n",
        "        2202: b'He',\n",
        "        344: b'llo',\n",
        "        4776: b',',\n",
        "        612: b' how',\n",
        "        3932: b' are',\n",
        "        50256: b'<|endoftext|>',  \n",
        "    }\n",
        "\n",
        "    merges = [\n",
        "        (b'H', b'e'),   # He\n",
        "        (b'e', b'l'),   # el \n",
        "        (b'l', b'l'),   # ll\n",
        "        (b'll', b'o'),  # llo\n",
        "    ]\n",
        "\n",
        "    bpe = OptimizedBPE(\n",
        "        vocab_size=len(vocab),\n",
        "        special_tokens=['<|endoftext|>']\n",
        "    )\n",
        "    bpe.initialize_from_existing(vocab, merges)\n",
        "\n",
        "    test_text = \"Hello, how are you?\"\n",
        "    tokens = bpe.tokenize(test_text)\n",
        "    print(f\"text: {test_text}\")\n",
        "    print(f\"bpe: {tokens}\")\n",
        "\n",
        "    ids = bpe.encode(test_text)\n",
        "    print(f\"code: {ids}\")\n",
        "    print(f\"decode: {bpe.decode(ids)}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_gpt2_compatibility()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## problem4.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iimport time\n",
        "import tracemalloc\n",
        "import cProfile\n",
        "import pstats\n",
        "import os\n",
        "from bpetraining import train_bpe\n",
        "\n",
        "def main():\n",
        "    print(\"=== Question 4(a): BPE Training ===\")\n",
        "\n",
        "    # Training parameters - file is in current directory\n",
        "    input_file = \"TinyStoriesV2-GPT4-valid.txt\"\n",
        "    vocab_size = 5000\n",
        "    special_tokens = [\"<|endoftext|>\"]\n",
        "\n",
        "    # Check if file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: File does not exist: {input_file}\")\n",
        "        print(\"Current directory:\", os.getcwd())\n",
        "        print(\"Directory contents:\", os.listdir(\".\"))\n",
        "        return\n",
        "\n",
        "    file_size = os.path.getsize(input_file) / (1024*1024)\n",
        "    print(f\"Using file: {input_file}\")\n",
        "    print(f\"File size: {file_size:.1f} MB\")\n",
        "    print(f\"Target vocabulary size: {vocab_size}\")\n",
        "    print(f\"Special tokens: {special_tokens}\")\n",
        "\n",
        "    # 1. Training with monitoring\n",
        "    print(\"\\nStarting training...\")\n",
        "    tracemalloc.start()\n",
        "    start_time = time.time()\n",
        "\n",
        "    vocab, merges = train_bpe(input_file, vocab_size, special_tokens)\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "\n",
        "    # 2. Result analysis\n",
        "    print(\"\\n=== Training completed! ===\")\n",
        "    print(f\"Training time: {training_time:.2f} seconds\")\n",
        "    print(f\"Peak memory usage: {peak_mem / 1024 / 1024:.2f} MB\")\n",
        "    print(f\"Vocabulary size: {len(vocab)}\")\n",
        "    print(f\"Number of merges: {len(merges)}\")\n",
        "\n",
        "    # Find longest token\n",
        "    max_len = 0\n",
        "    longest_tokens = []\n",
        "    for token_id, token_bytes in vocab.items():\n",
        "        if token_id >= 257:  # Skip single bytes and special tokens\n",
        "            token_len = len(token_bytes)\n",
        "            if token_len > max_len:\n",
        "                max_len = token_len\n",
        "                longest_tokens = [(token_id, token_bytes)]\n",
        "            elif token_len == max_len:\n",
        "                longest_tokens.append((token_id, token_bytes))\n",
        "\n",
        "    print(f\"\\nLongest token length: {max_len} bytes\")\n",
        "    if longest_tokens:\n",
        "        token_id, token_bytes = longest_tokens[0]\n",
        "        try:\n",
        "            token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "            print(f\"Example longest token (ID={token_id}): {repr(token_str)}\")\n",
        "            print(f\"Hexadecimal: {token_bytes.hex()}\")\n",
        "        except:\n",
        "            print(f\"Example longest token (ID={token_id}): Cannot decode as UTF-8\")\n",
        "            print(f\"Hexadecimal: {token_bytes.hex()}\")\n",
        "\n",
        "    # 3. Performance analysis\n",
        "    print(\"\\n=== Question 4(b): Performance Analysis ===\")\n",
        "    print(\"Running performance analysis...\")\n",
        "\n",
        "    profiler = cProfile.Profile()\n",
        "    profiler.enable()\n",
        "\n",
        "    # Run again for profiling\n",
        "    vocab2, merges2 = train_bpe(input_file, vocab_size, special_tokens)\n",
        "\n",
        "    profiler.disable()\n",
        "\n",
        "    # Output analysis results\n",
        "    stats = pstats.Stats(profiler)\n",
        "    print(\"\\nTop 10 functions by cumulative time:\")\n",
        "    stats.strip_dirs().sort_stats('cumulative').print_stats(10)\n",
        "\n",
        "    print(\"\\nTop 10 functions by internal time:\")\n",
        "    stats.sort_stats('time').print_stats(10)\n",
        "\n",
        "    # Save analysis results\n",
        "    profiler.dump_stats(\"bpe_performance.prof\")\n",
        "    print(f\"\\nDetailed performance analysis saved to: bpe_performance.prof\")\n",
        "    print(\"Use 'snakeviz bpe_performance.prof' for visualization\")\n",
        "\n",
        "    # Generate answer file\n",
        "    with open(\"problem4_answer.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"# Question 4 Answer\\n\\n\")\n",
        "        f.write(\"## (a) BPE Training Results\\n\")\n",
        "        f.write(f\"- Training file: TinyStoriesV2-GPT4-valid.txt ({file_size:.1f}MB)\\n\")\n",
        "        f.write(f\"- Target vocab_size: {vocab_size}\\n\")\n",
        "        f.write(f\"- Actual vocab_size: {len(vocab)}\\n\")\n",
        "        f.write(f\"- Training time: {training_time:.2f} seconds\\n\")\n",
        "        f.write(f\"- Peak memory usage: {peak_mem / 1024 / 1024:.2f} MB\\n\")\n",
        "        f.write(f\"- Longest token length: {max_len} bytes\\n\")\n",
        "        if longest_tokens:\n",
        "            token_id, token_bytes = longest_tokens[0]\n",
        "            try:\n",
        "                token_str = token_bytes.decode('utf-8', errors='replace')\n",
        "                f.write(f\"- Longest token example: {repr(token_str)}\\n\")\n",
        "            except:\n",
        "                f.write(f\"- Longest token example: (Cannot decode as UTF-8)\\n\")\n",
        "\n",
        "        f.write(\"\\n## (b) Performance Analysis\\n\")\n",
        "        f.write(\"According to profiler output, the most time-consuming parts are usually:\\n\")\n",
        "        f.write(\"1. Merge operations in the main loop (while current_vocab_size < vocab_size and pair_counts:)\\n\")\n",
        "        f.write(\"2. Loop for finding the best pair (for pair, cnt in pair_counts.items():)\\n\")\n",
        "        f.write(\"3. Updating pair_counts and pair_to_word_indices dictionaries\\n\")\n",
        "        f.write(\"4. Text preprocessing (_pretokenize_chunk function)\\n\")\n",
        "\n",
        "    print(f\"\\nAnswer saved to: problem4_answer.txt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
